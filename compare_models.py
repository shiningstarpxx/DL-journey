"""
æ¨¡å‹å¯¹æ¯”è„šæœ¬
å¯¹æ¯”AlexNetã€Modern-AlexNetã€LeNetã€Modern-LeNetå››ä¸ªæ¨¡å‹åœ¨CIFAR-10æ•°æ®é›†ä¸Šçš„æ€§èƒ½
"""

import os
import sys
import torch
import logging
import time
import json
from tqdm import tqdm
import matplotlib.pyplot as plt
import numpy as np
import matplotlib
from matplotlib import font_manager

# è®¾ç½®ä¸­æ–‡å­—ä½“ï¼Œè§£å†³ä¸­æ–‡æ˜¾ç¤ºé—®é¢˜
def setup_chinese_font():
    """è®¾ç½®ä¸­æ–‡å­—ä½“"""
    try:
        # å°è¯•è®¾ç½®ä¸­æ–‡å­—ä½“
        plt.rcParams['font.sans-serif'] = ['SimHei', 'Arial Unicode MS', 'DejaVu Sans', 'PingFang SC', 'Hiragino Sans GB']
        plt.rcParams['axes.unicode_minus'] = False
        
        # æ£€æŸ¥å­—ä½“æ˜¯å¦å¯ç”¨
        font_list = font_manager.findSystemFonts()
        chinese_fonts = []
        for font in font_list:
            try:
                font_name = font_manager.FontProperties(fname=font).get_name().lower()
                if any(name in font_name for name in ['simhei', 'arial unicode', 'dejavu', 'pingfang', 'hiragino']):
                    chinese_fonts.append(font)
            except:
                continue
        
        if chinese_fonts:
            # ä½¿ç”¨ç¬¬ä¸€ä¸ªå¯ç”¨çš„ä¸­æ–‡å­—ä½“
            font_path = chinese_fonts[0]
            font_prop = font_manager.FontProperties(fname=font_path)
            plt.rcParams['font.family'] = font_prop.get_name()
            print(f"âœ… å·²è®¾ç½®ä¸­æ–‡å­—ä½“: {font_prop.get_name()}")
        else:
            print("âš ï¸ æœªæ‰¾åˆ°ä¸­æ–‡å­—ä½“ï¼Œå›¾è¡¨ä¸­çš„ä¸­æ–‡å¯èƒ½æ— æ³•æ­£å¸¸æ˜¾ç¤º")
            # å°è¯•ä½¿ç”¨ç³»ç»Ÿé»˜è®¤å­—ä½“
            plt.rcParams['font.family'] = 'sans-serif'
            
    except Exception as e:
        print(f"âš ï¸ è®¾ç½®ä¸­æ–‡å­—ä½“æ—¶å‡ºé”™: {e}")
        print("å›¾è¡¨ä¸­çš„ä¸­æ–‡å¯èƒ½æ— æ³•æ­£å¸¸æ˜¾ç¤º")

# è®¾ç½®æ—¥å¿—
logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')
logger = logging.getLogger(__name__)

class ModelComparison:
    """æ¨¡å‹å¯¹æ¯”ç±»"""
    
    def __init__(self):
        # å¯¼å…¥é¡¹ç›®æ¨¡å—
        from models.alexnet import AlexNet, AlexNetModern
        from models.lenet import LeNet, LeNetModern
        from datasets.cifar import CIFAR10Dataset
        from utils.device import get_best_device, optimize_for_device
        from utils.metrics import calculate_accuracy
        
        # ä¿å­˜å¯¼å…¥çš„å‡½æ•°
        self.calculate_accuracy = calculate_accuracy
        
        # å¼ºåˆ¶ä½¿ç”¨CPUä»¥é¿å…MPSå…¼å®¹æ€§é—®é¢˜
        self.device = torch.device("cpu")
        logger.info(f"ä½¿ç”¨è®¾å¤‡: {self.device}")
        
        # åˆ›å»ºæ•°æ®é›†ï¼ˆä½¿ç”¨è¾ƒå°çš„batch_sizeå’Œæ›´å°‘çš„æ•°æ®ï¼‰
        logger.info("ğŸ“Š åŠ è½½CIFAR-10æ•°æ®é›†...")
        self.dataset = CIFAR10Dataset(batch_size=64, num_workers=0)
        self.train_loader, self.test_loader = self.dataset.get_dataloaders()
        
        # åˆ›å»ºå››ä¸ªæ¨¡å‹
        logger.info("ğŸ“¦ åˆ›å»ºæ¨¡å‹...")
        self.models = {
            'AlexNet': AlexNet(num_classes=10),
            'Modern-AlexNet': AlexNetModern(num_classes=10),
            'LeNet': LeNet(num_classes=10, input_channels=3),  # æ”¯æŒå½©è‰²å›¾åƒ
            'Modern-LeNet': LeNetModern(num_classes=10, input_channels=3)
        }
        
        # ä¼˜åŒ–æ¨¡å‹åˆ°è®¾å¤‡
        for name, model in self.models.items():
            self.models[name] = optimize_for_device(model, self.device)
            logger.info(f"{name}å‚æ•°æ•°é‡: {model.count_parameters():,}")
            logger.info(f"{name}æ¨¡å‹å¤§å°: {model.get_model_size_mb():.2f} MB")
        
        # è®­ç»ƒé…ç½®ï¼ˆå‡å°‘è®­ç»ƒæ—¶é—´ï¼‰
        self.epochs = 3  # å‡å°‘åˆ°3ä¸ªepoch
        self.learning_rate = 0.001
        
        # å­˜å‚¨è®­ç»ƒå†å²
        self.history = {
            name: {'train_loss': [], 'train_acc': [], 'val_loss': [], 'val_acc': []}
            for name in self.models.keys()
        }
    
    def train_model(self, model, model_name):
        """è®­ç»ƒå•ä¸ªæ¨¡å‹"""
        logger.info(f"ğŸ¯ å¼€å§‹è®­ç»ƒ {model_name}...")
        
        optimizer = torch.optim.Adam(model.parameters(), lr=self.learning_rate)
        criterion = torch.nn.CrossEntropyLoss()
        
        train_losses = []
        train_accs = []
        val_losses = []
        val_accs = []
        
        for epoch in range(self.epochs):
            # è®­ç»ƒé˜¶æ®µ
            model.train()
            total_loss = 0
            total_correct = 0
            total_samples = 0
            
            # åªè®­ç»ƒéƒ¨åˆ†æ•°æ®ä»¥èŠ‚çœæ—¶é—´
            train_iter = iter(self.train_loader)
            num_batches = min(100, len(self.train_loader))  # é™åˆ¶è®­ç»ƒæ‰¹æ¬¡æ•°é‡
            
            progress_bar = tqdm(range(num_batches), desc=f'{model_name} Epoch {epoch+1}/{self.epochs}')
            
            for _ in progress_bar:
                try:
                    data, target = next(train_iter)
                except StopIteration:
                    train_iter = iter(self.train_loader)
                    data, target = next(train_iter)
                
                data, target = data.to(self.device), target.to(self.device)
                
                optimizer.zero_grad()
                output = model(data)
                loss = criterion(output, target)
                loss.backward()
                optimizer.step()
                
                total_loss += loss.item()
                total_correct += self.calculate_accuracy(output, target) * target.size(0)
                total_samples += target.size(0)
                
                progress_bar.set_postfix({
                    'Loss': f'{loss.item():.4f}',
                    'Acc': f'{self.calculate_accuracy(output, target):.2%}'
                })
            
            avg_train_loss = total_loss / num_batches
            avg_train_acc = total_correct / total_samples
            train_losses.append(avg_train_loss)
            train_accs.append(avg_train_acc)
            
            # éªŒè¯é˜¶æ®µ
            model.eval()
            total_val_loss = 0
            total_val_correct = 0
            total_val_samples = 0
            
            with torch.no_grad():
                # åªéªŒè¯éƒ¨åˆ†æ•°æ®
                val_iter = iter(self.test_loader)
                num_val_batches = min(50, len(self.test_loader))
                
                for _ in range(num_val_batches):
                    try:
                        data, target = next(val_iter)
                    except StopIteration:
                        val_iter = iter(self.test_loader)
                        data, target = next(val_iter)
                    
                    data, target = data.to(self.device), target.to(self.device)
                    output = model(data)
                    loss = criterion(output, target)
                    
                    total_val_loss += loss.item()
                    total_val_correct += self.calculate_accuracy(output, target) * target.size(0)
                    total_val_samples += target.size(0)
            
            avg_val_loss = total_val_loss / num_val_batches
            avg_val_acc = total_val_correct / total_val_samples
            val_losses.append(avg_val_loss)
            val_accs.append(avg_val_acc)
            
            logger.info(f'{model_name} Epoch {epoch+1}: Train Loss: {avg_train_loss:.4f}, Train Acc: {avg_train_acc:.2%}, Val Loss: {avg_val_loss:.4f}, Val Acc: {avg_val_acc:.2%}')
        
        return train_losses, train_accs, val_losses, val_accs
    
    def compare_models(self):
        """å¯¹æ¯”æ¨¡å‹æ€§èƒ½"""
        logger.info("ğŸš€ å¼€å§‹æ¨¡å‹å¯¹æ¯”...")
        
        # è®­ç»ƒæ‰€æœ‰æ¨¡å‹
        for model_name, model in self.models.items():
            train_loss, train_acc, val_loss, val_acc = self.train_model(model, model_name)
            
            # å­˜å‚¨ç»“æœ
            self.history[model_name]['train_loss'] = train_loss
            self.history[model_name]['train_acc'] = train_acc
            self.history[model_name]['val_loss'] = val_loss
            self.history[model_name]['val_acc'] = val_acc
        
        # æœ€ç»ˆè¯„ä¼°
        self.final_evaluation()
        
        # ç»˜åˆ¶å¯¹æ¯”å›¾
        self.plot_comparison()
        
        # ä¿å­˜ç»“æœ
        self.save_results()
    
    def final_evaluation(self):
        """æœ€ç»ˆè¯„ä¼°"""
        logger.info("ğŸ” æœ€ç»ˆè¯„ä¼°...")
        
        results = {}
        
        for model_name, model in self.models.items():
            model.eval()
            total_correct = 0
            total_samples = 0
            inference_times = []
            
            with torch.no_grad():
                # åªè¯„ä¼°éƒ¨åˆ†æµ‹è¯•æ•°æ®
                test_iter = iter(self.test_loader)
                num_test_batches = min(100, len(self.test_loader))
                
                for _ in tqdm(range(num_test_batches), desc=f'Evaluating {model_name}'):
                    try:
                        data, target = next(test_iter)
                    except StopIteration:
                        test_iter = iter(self.test_loader)
                        data, target = next(test_iter)
                    
                    data, target = data.to(self.device), target.to(self.device)
                    
                    # æµ‹é‡æ¨ç†æ—¶é—´
                    start_time = time.time()
                    output = model(data)
                    inference_time = time.time() - start_time
                    inference_times.append(inference_time)
                    
                    total_correct += self.calculate_accuracy(output, target) * target.size(0)
                    total_samples += target.size(0)
            
            accuracy = total_correct / total_samples
            avg_inference_time = np.mean(inference_times)
            
            results[model_name] = {
                'final_accuracy': accuracy,
                'avg_inference_time': avg_inference_time,
                'model_params': model.count_parameters(),
                'model_size_mb': model.get_model_size_mb()
            }
            
            logger.info(f"{model_name} - æœ€ç»ˆå‡†ç¡®ç‡: {accuracy:.2%}, å¹³å‡æ¨ç†æ—¶é—´: {avg_inference_time:.4f}s")
        
        self.final_results = results
    
    def plot_comparison(self):
        """ç»˜åˆ¶å¯¹æ¯”å›¾"""
        logger.info("ğŸ“Š ç»˜åˆ¶å¯¹æ¯”å›¾...")
        
        # è®¾ç½®ä¸­æ–‡å­—ä½“
        setup_chinese_font()
        
        fig, ((ax1, ax2), (ax3, ax4)) = plt.subplots(2, 2, figsize=(16, 12))
        
        epochs = range(1, self.epochs + 1)
        colors = ['blue', 'red', 'green', 'orange']
        
        # è®­ç»ƒæŸå¤±å¯¹æ¯”
        for i, (name, history) in enumerate(self.history.items()):
            ax1.plot(epochs, history['train_loss'], color=colors[i], label=name, linewidth=2, marker='o')
        ax1.set_title('è®­ç»ƒæŸå¤±å¯¹æ¯”', fontsize=14, fontweight='bold')
        ax1.set_xlabel('Epoch', fontsize=12)
        ax1.set_ylabel('Loss', fontsize=12)
        ax1.legend(fontsize=10)
        ax1.grid(True, alpha=0.3)
        
        # è®­ç»ƒå‡†ç¡®ç‡å¯¹æ¯”
        for i, (name, history) in enumerate(self.history.items()):
            ax2.plot(epochs, history['train_acc'], color=colors[i], label=name, linewidth=2, marker='o')
        ax2.set_title('è®­ç»ƒå‡†ç¡®ç‡å¯¹æ¯”', fontsize=14, fontweight='bold')
        ax2.set_xlabel('Epoch', fontsize=12)
        ax2.set_ylabel('Accuracy', fontsize=12)
        ax2.legend(fontsize=10)
        ax2.grid(True, alpha=0.3)
        
        # éªŒè¯æŸå¤±å¯¹æ¯”
        for i, (name, history) in enumerate(self.history.items()):
            ax3.plot(epochs, history['val_loss'], color=colors[i], label=name, linewidth=2, marker='o')
        ax3.set_title('éªŒè¯æŸå¤±å¯¹æ¯”', fontsize=14, fontweight='bold')
        ax3.set_xlabel('Epoch', fontsize=12)
        ax3.set_ylabel('Loss', fontsize=12)
        ax3.legend(fontsize=10)
        ax3.grid(True, alpha=0.3)
        
        # éªŒè¯å‡†ç¡®ç‡å¯¹æ¯”
        for i, (name, history) in enumerate(self.history.items()):
            ax4.plot(epochs, history['val_acc'], color=colors[i], label=name, linewidth=2, marker='o')
        ax4.set_title('éªŒè¯å‡†ç¡®ç‡å¯¹æ¯”', fontsize=14, fontweight='bold')
        ax4.set_xlabel('Epoch', fontsize=12)
        ax4.set_ylabel('Accuracy', fontsize=12)
        ax4.legend(fontsize=10)
        ax4.grid(True, alpha=0.3)
        
        plt.tight_layout()
        plt.savefig('model_comparison.png', dpi=300, bbox_inches='tight')
        logger.info("å¯¹æ¯”å›¾å·²ä¿å­˜ä¸º model_comparison.png")
        
        # ç»˜åˆ¶æ€§èƒ½å¯¹æ¯”æŸ±çŠ¶å›¾
        self.plot_performance_comparison()
    
    def plot_performance_comparison(self):
        """ç»˜åˆ¶æ€§èƒ½å¯¹æ¯”æŸ±çŠ¶å›¾"""
        logger.info("ğŸ“Š ç»˜åˆ¶æ€§èƒ½å¯¹æ¯”æŸ±çŠ¶å›¾...")
        
        # è®¾ç½®ä¸­æ–‡å­—ä½“
        setup_chinese_font()
        
        fig, ((ax1, ax2), (ax3, ax4)) = plt.subplots(2, 2, figsize=(16, 12))
        
        model_names = list(self.final_results.keys())
        accuracies = [self.final_results[name]['final_accuracy'] for name in model_names]
        inference_times = [self.final_results[name]['avg_inference_time'] for name in model_names]
        params = [self.final_results[name]['model_params'] for name in model_names]
        sizes = [self.final_results[name]['model_size_mb'] for name in model_names]
        
        colors = ['skyblue', 'lightcoral', 'lightgreen', 'orange']
        
        # å‡†ç¡®ç‡å¯¹æ¯”
        bars1 = ax1.bar(model_names, accuracies, color=colors, alpha=0.8)
        ax1.set_title('æœ€ç»ˆå‡†ç¡®ç‡å¯¹æ¯”', fontsize=14, fontweight='bold')
        ax1.set_ylabel('Accuracy', fontsize=12)
        ax1.set_ylim(0, max(accuracies) * 1.1)
        # åœ¨æŸ±å­ä¸Šæ·»åŠ æ•°å€¼æ ‡ç­¾
        for bar, acc in zip(bars1, accuracies):
            height = bar.get_height()
            ax1.text(bar.get_x() + bar.get_width()/2., height + 0.01,
                    f'{acc:.2%}', ha='center', va='bottom', fontweight='bold')
        
        # æ¨ç†æ—¶é—´å¯¹æ¯”
        bars2 = ax2.bar(model_names, inference_times, color=colors, alpha=0.8)
        ax2.set_title('å¹³å‡æ¨ç†æ—¶é—´å¯¹æ¯”', fontsize=14, fontweight='bold')
        ax2.set_ylabel('Time (seconds)', fontsize=12)
        # åœ¨æŸ±å­ä¸Šæ·»åŠ æ•°å€¼æ ‡ç­¾
        for bar, time_val in zip(bars2, inference_times):
            height = bar.get_height()
            ax2.text(bar.get_x() + bar.get_width()/2., height + 0.0001,
                    f'{time_val:.4f}s', ha='center', va='bottom', fontweight='bold')
        
        # å‚æ•°æ•°é‡å¯¹æ¯”
        bars3 = ax3.bar(model_names, params, color=colors, alpha=0.8)
        ax3.set_title('æ¨¡å‹å‚æ•°æ•°é‡å¯¹æ¯”', fontsize=14, fontweight='bold')
        ax3.set_ylabel('Parameters', fontsize=12)
        # åœ¨æŸ±å­ä¸Šæ·»åŠ æ•°å€¼æ ‡ç­¾
        for bar, param in zip(bars3, params):
            height = bar.get_height()
            ax3.text(bar.get_x() + bar.get_width()/2., height + max(params) * 0.01,
                    f'{param:,}', ha='center', va='bottom', fontweight='bold', fontsize=8)
        
        # æ¨¡å‹å¤§å°å¯¹æ¯”
        bars4 = ax4.bar(model_names, sizes, color=colors, alpha=0.8)
        ax4.set_title('æ¨¡å‹å¤§å°å¯¹æ¯”', fontsize=14, fontweight='bold')
        ax4.set_ylabel('Size (MB)', fontsize=12)
        # åœ¨æŸ±å­ä¸Šæ·»åŠ æ•°å€¼æ ‡ç­¾
        for bar, size in zip(bars4, sizes):
            height = bar.get_height()
            ax4.text(bar.get_x() + bar.get_width()/2., height + 0.01,
                    f'{size:.2f}MB', ha='center', va='bottom', fontweight='bold')
        
        # æ—‹è½¬xè½´æ ‡ç­¾ä»¥é¿å…é‡å 
        for ax in [ax1, ax2, ax3, ax4]:
            ax.tick_params(axis='x', rotation=45)
            ax.grid(True, alpha=0.3)
        
        plt.tight_layout()
        plt.savefig('performance_comparison.png', dpi=300, bbox_inches='tight')
        logger.info("æ€§èƒ½å¯¹æ¯”å›¾å·²ä¿å­˜ä¸º performance_comparison.png")
    
    def save_results(self):
        """ä¿å­˜ç»“æœ"""
        logger.info("ğŸ’¾ ä¿å­˜ç»“æœ...")
        
        results = {
            'final_results': self.final_results,
            'training_history': self.history,
            'model_info': {
                name: {
                    'parameters': model.count_parameters(),
                    'size_mb': model.get_model_size_mb()
                }
                for name, model in self.models.items()
            }
        }
        
        with open('comparison_results.json', 'w', encoding='utf-8') as f:
            json.dump(results, f, indent=2, ensure_ascii=False)
        
        logger.info("ç»“æœå·²ä¿å­˜ä¸º comparison_results.json")
    
    def print_summary(self):
        """æ‰“å°å¯¹æ¯”æ‘˜è¦"""
        print("\n" + "="*80)
        print("æ¨¡å‹å¯¹æ¯”æ‘˜è¦")
        print("="*80)
        
        # åˆ›å»ºè¡¨æ ¼å¤´éƒ¨
        print(f"{'æ¨¡å‹åç§°':<15} {'å‡†ç¡®ç‡':<10} {'æ¨ç†æ—¶é—´(s)':<12} {'å‚æ•°æ•°é‡':<12} {'æ¨¡å‹å¤§å°(MB)':<12}")
        print("-" * 80)
        
        # æŒ‰å‡†ç¡®ç‡æ’åº
        sorted_results = sorted(self.final_results.items(), 
                              key=lambda x: x[1]['final_accuracy'], reverse=True)
        
        for model_name, results in sorted_results:
            acc = results['final_accuracy']
            time_val = results['avg_inference_time']
            params = results['model_params']
            size = results['model_size_mb']
            
            print(f"{model_name:<15} {acc:.2%} {time_val:.4f}s {params:,} {size:.2f}")
        
        print("-" * 80)
        
        # æ‰¾å‡ºæœ€ä½³æ¨¡å‹
        best_model = sorted_results[0]
        print(f"\nğŸ† æœ€ä½³å‡†ç¡®ç‡æ¨¡å‹: {best_model[0]} ({best_model[1]['final_accuracy']:.2%})")
        
        # æ‰¾å‡ºæœ€å¿«æ¨¡å‹
        fastest_model = min(self.final_results.items(), 
                          key=lambda x: x[1]['avg_inference_time'])
        print(f"âš¡ æœ€å¿«æ¨ç†æ¨¡å‹: {fastest_model[0]} ({fastest_model[1]['avg_inference_time']:.4f}s)")
        
        # æ‰¾å‡ºæœ€å°æ¨¡å‹
        smallest_model = min(self.final_results.items(), 
                           key=lambda x: x[1]['model_size_mb'])
        print(f"ğŸ“¦ æœ€å°æ¨¡å‹: {smallest_model[0]} ({smallest_model[1]['model_size_mb']:.2f}MB)")
        
        print("\nç»“è®º:")
        print("1. ç°ä»£ç‰ˆæœ¬æ¨¡å‹é€šå¸¸å…·æœ‰æ›´å¥½çš„æ€§èƒ½å’Œæ›´å¿«çš„æ”¶æ•›é€Ÿåº¦")
        print("2. AlexNetç³»åˆ—æ¨¡å‹å‚æ•°æ›´å¤šï¼Œä½†é€šå¸¸èƒ½è·å¾—æ›´é«˜çš„å‡†ç¡®ç‡")
        print("3. LeNetç³»åˆ—æ¨¡å‹æ›´è½»é‡ï¼Œæ¨ç†é€Ÿåº¦æ›´å¿«")
        print("4. ç°ä»£ç‰ˆæœ¬é€šè¿‡BatchNormç­‰æŠ€æœ¯æé«˜äº†è®­ç»ƒç¨³å®šæ€§")
        print("="*80)

def main():
    """ä¸»å‡½æ•°"""
    try:
        # ç¡®ä¿åœ¨è™šæ‹Ÿç¯å¢ƒä¸­è¿è¡Œ
        import torch
        print(f"PyTorchç‰ˆæœ¬: {torch.__version__}")
        print(f"MPSå¯ç”¨: {torch.backends.mps.is_available()}")
        
        # åˆ›å»ºå¯¹æ¯”å™¨å¹¶è¿è¡Œå¯¹æ¯”
        comparator = ModelComparison()
        comparator.compare_models()
        comparator.print_summary()
        
        print("\nğŸ‰ æ¨¡å‹å¯¹æ¯”å®Œæˆï¼")
        print("ğŸ“Š æŸ¥çœ‹ model_comparison.png è·å–è®­ç»ƒæ›²çº¿å¯¹æ¯”")
        print("ğŸ“Š æŸ¥çœ‹ performance_comparison.png è·å–æ€§èƒ½å¯¹æ¯”")
        print("ğŸ“„ æŸ¥çœ‹ comparison_results.json è·å–è¯¦ç»†æ•°æ®")
        
    except ImportError as e:
        print(f"âŒ å¯¼å…¥é”™è¯¯: {e}")
        print("è¯·ç¡®ä¿å·²æ¿€æ´»è™šæ‹Ÿç¯å¢ƒå¹¶å®‰è£…äº†æ‰€æœ‰ä¾èµ–")
    except Exception as e:
        print(f"âŒ è¿è¡Œé”™è¯¯: {e}")
        import traceback
        traceback.print_exc()

if __name__ == '__main__':
    main()
